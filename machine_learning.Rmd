---
title: "Machine learning"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Supervised learning 
Supervised learning handles labeled data and mostly used for prediction. THe most simple case would be linear regression, where we have a standard curve, and we use the standard curve to predict experimental results. 

## Linear regression
```{r}
library(stats)
model= lm(formular=Y~X, data=df)

summary(model)
coef(model)
residuals(model)  #mean(residuals(model))==0
df.residuals(model)

SSE= sqrt(sum(residuals(model)^2))
RMSE= sqrt(sum(residuals(model)^2)/df.residual(model))
SST=sqrt(sum((y-mean(y))^2))

R^2= 1-SSE/SST = 1- var(e)/var(y) #The percentage of variation in Y that can be explained by the model. 

```
Often we use linear regression to make categorical prediction. In this case, we can turn scatter plot into box plot use the ```cut``` function. 
```{r}
x=cut(x, breaks = n) #number of catagory
```

## non-linear regression 

Non-linear regression use glm function, in addition to formular and dataframe, we also need to specify the ```family``` of distribution to use. Three common distributions are binomial, gaussian, and Possion. The binomial distribution describes the number of positive outcomes in binary experiments, and it is the “mother” distribution from which the other two distributions can be obtained. The Gaussian distribution can be considered as a special case of the binomial, when the number of tries is sufficiently large. For this reason, the Gaussian distribution applies to a large number of variables, and it is referred to as the normal distribution. The Poisson distribution applies to counting experiments, and it can be obtained as the limit of the binomial distribution when the probability of success is small.

The ```type``` argument in ```predict``` function indicates the type of prediction required. The default is "links", which shows on the scale of the linear predictors-- thus for lm model you don't need to specify the type; the alternative "response" is on the scale of the response variable. Thus for a default binomial model the default predictions are of log-odds (probabilities on logit scale) and type = "response" gives the predicted probabilities. The "terms" option returns a matrix giving the fitted values of each term in the model formula on the linear predictor scale.

```{r}
model=glm(y~x1+x2+x3, data=df, family="gaussian") #gaussian is the default. 

#all types of non-linear regressions are glm, you can specify family: binomial, gaussian, Gamma (inverse), inverse.gaussian, poisson(link = "log"), quasi,quasibinomial,quasipoisson

probability=predict(model, df,type="response") #apply the model to data

# Example 1, use "binomial" regression to make binary prediction. 
model=glm(y~x1+x2+x3, data=df, family="binomial")
probability = predict(model, new_df, type="response")
prediction = ifelse(probability>0.5, 1, 0) #note that the cut-off is chosen arbitrarily and is context dependent. 
mean(probability==prediction) #to evaluate how well the model works.  
```

## Tree-based regression (supervised)
```{r }

```

#classification

#Clustering-based classification (supervised and unsupervised) 
cluster are based on the similarity of each samples in the training set. It calculates the distance among all points (a scatter plot). *distance= 1- similarity*.Imaging there are two parameters, then each data point can be translate to one point on a scatter plot and we can measure the spacial distance of two points. Distance/cluster based classification can be used in both supervised and unsupervised learning. 

```{r}
matrix_height_weight<- (5.5, 120, 5.6, 130, 6, 170, nrow=3)
dist_hw<-dist(matrix_height_weight, method= 'euclidean')
#there are also 'baltimore' and 'manhattan' distance. look into hierarchical clustering to find more. 
#dist() creates a triangular matrix where there is a number for each pair of points in the originall matrix. 

scale(dist_hw) #This would normalize each parameter so that they contribute the same way in the distance calculation. 
```

Below are some commonly cluster-based methods: 

#Supervised cluster

We have labelled training dataset (eg. subtype of a disease for diagnosis) and would like to assign labels. The *knn* cluuster and *decision tree* are two common cluster methods. 

##knn cluster (k nearest neighbour, supervised learning)
The number of cluster is abitrarily assigned, and the seed is chosen randomly. For each data point, we classify it into one of the cluster based on the location of this point on the scatter plot. For knn method, we use the closest k neighbors to vote on which cluster a point belongs to. k is a number you assigned to the program. When k= 1 then every point will be assigned based on the closest neighbour. A rule of thumb is that use sqrt(total data in training set) as k.

Note that as a distance-based method, knn requires all parameters to be numeric. You'll need to first convert categorical data. Also you should normalize all parameters (eg, height and weight are in different unit) as it will use all parameters to calculate the difference. 

```{r}
library(class)
knn(train=df_training[,-type], test=df_test, cl=type, k=1, prob=F) 
#type is the column that shows the class of each training set; k=1 is default; default prob=F will assign the testing data based on the majority of votes. prob=T will return the propotion of the votes for the winning class in probability.
predict<-knn(train=df_training, test=df_test, cl=type, k=1, prob=F)
prob<-attr(predict, "prob")

normalize<-function(x){
  return((x-min(x))/(max(x)-min(x)))
}
```
## decision tree and random forest (supervised learning)


# Unsupervised clustering

## kmeans clustering (unsupervised learning)
For unsupervised learning, the dataset has no labels- for example, you have customer datas, how to you group the customers into different segments? kmeans use the mean value of a number of points position as a "centeroid" and assign points to centeroids. You need to assign the number of clusters  (normally by first plot a scatter plot and look at how many centers are there)
```{r}
library(stats)
model<-kmeans(df, centers=k, nstart=1)

#To evaluate, we calculate the sum of square distance among each point to the centeroid of the cluster it belongs to. If we don't know what should be the ideal k, use elbow plot to compare:
library(purrr)
tot_withinss <- map_dbl(1:10, function(k){
  model <- kmeans(x = lineup, centers = k) 
  model$tot.withinss 
})
#try k in 1:10 and return the sum of squares.
elbow_df <- data.frame(
k = 1:10,
tot_withinss = tot_withinss)
ggplot(elbow_df, aes(x = k, y = tot_withinss)) + geom_line()
#plot, the higher k is the lower the sum is, but we would use the turning point. 
```

##Hierarchical clustering (unsupervised)
Hierarchical clustering is a distance-based unsupervised learning method. Similar to kmeans, we also use a scatter plot to visualize a matrix, and use the relative position of each point to cluster. An example would be a phylogenetic tree. First, we use *dist* function to calculate the distance of each point, then use *hclust* to generate a tree. Finally we can cut the tree to generate clusters. 

```{r}
dist_player <- dist(player, method = 'euclidean') 
#euclidean caulculates the distance between the two points. "jaccard" method can be used for categoriical data (T or F; A,B,C etc)
hc_player <- hclust(dist_player, method = 'complete')
# Method could me complete, single, average or centroid. "Complete" use pairwise similarity between all observations in cluster 1 and 2 and use the largest of similarities; "single" use the smallest similarities and average use the average similarities. Centroid find the centroid of each cluster. "Single" turns to create unbalanced tree and it's good to use it to call out outliers in a dataset. "Complete" creates most balanced tree and is most common. 
plot(hc_player) #can draw a tree
cutree(hc_player, h=6)
cutree(hc_player, k=3)
#cut tree by height (h, the distance) or by number of clusters (k) 
```

##PCA (unsupervised)
Principle Component Analysis is an unsupervised learning method based on dimensionality reduction. It is a specific way of singular value decomposition (SVD). Often we have n variables that are not *independent*. PCA creates a new set of variable << n that are independent of each other.

We don't know the relationship between old n variables (eg, age, gender, height, weight, etc) and new set oof variables (Component 1,2,3, etc). We do know the percentage of each component. 

```{r}
pr.df<- prcomp(df, scale=TRUE, center=TRUE)
summary(pr.df) #a table of proportion of variance
biplot(pr.df) #shows how each of the original variable locates based on new components

#to evaluate how many components are important to interpret the data, we can use scree plot variance explained for each principal component 
pr.var <- pr.df$sdev^2
pve <- pr.var / sum(pr.var)
plot(pve)
```


